{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Beer recommendation\n",
    "\n",
    "## Where to place new data\n",
    "\n",
    "New data should be placed under the subfolder \"./new-beer-recipes-here/*\". \n",
    "\n",
    "IMPORTANT: The old data should still be placed under the subfolder \"./original-beer-recipes/*\" here, because the program needs to re-lear at each startup (I don't persist the learning to save time). Training may take some time (less than 5 minutes).\n",
    "\n",
    "## How to use\n",
    "\n",
    "Run this notebook with \"ipython notebook\", or \"jupyter notebook\", python 3 version. \n",
    "\n",
    "## Note on corrupted data\n",
    "\n",
    "The line 4070 (ID 4069) of the CSV is corrupted and doesn't have proper alignment of columns for some reason: \n",
    "`4069,DIPA, mango smoothie,/homebrew/recipe/view/435411/dipa-mango-smoothie,Double IPA,56,18.93,1.084,1.011,9.52,64.48,5.24,15.14,30,1.104,75,N/A,Specific Gravity,BIAB,0.5,21.11,Snowberry Honey,N/A,10,`. For example, when I ask for the column \"StyleID\" that should give me the value \"56\" here, I instead get the previous column \"Double IPA\". It is because there is 3 fields instead of 2 at the beginning, the error is here: `DIPA, mango smoothie` should not contain any comma. I fixed the line by deleting this comma instead of deleting the line: to keep more data for training. \n",
    "\n",
    "Note: There is also an extra comma at the end of every line, which creates an empty column. I ignore this column when processing the data. So the input is all first columns, output is the last column, and the next last column is ignored and deleted. \n",
    "\n",
    "Also note: it seems that not only the line 4070 is corrupted. Well, I'll simply drop a lot of columns below and process on just a subset of the data which I keep programmatically. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## We'll now load the data set in Python and then train decision trees with XGBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# python3\n",
    "# pip install xgboost\n",
    "# pip install numpy\n",
    "# pip install matplotlib\n",
    "# pip install scikit-learn\n",
    "# pip install pandas\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import xgboost as xgb\n",
    "from sklearn.preprocessing import LabelEncoder, OneHotEncoder\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "import os\n",
    "import glob\n",
    "\n",
    "\n",
    "# xgb = xgboost.XGBRegressor(n_estimators=100, learning_rate=0.08, gamma=0, subsample=0.75,\n",
    "#                            colsample_bytree=1, max_depth=7)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(73691, 11)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/IPython/core/interactiveshell.py:2903: DtypeWarning: Columns (4,18,22) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  if self.run_code(code, result):\n"
     ]
    }
   ],
   "source": [
    "def load_categories():\n",
    "    relative_data_path = './original-beer-recipes/styleData.csv'\n",
    "    pdtrain = pd.read_csv(relative_data_path, index_col='BeerID', encoding='latin1')\n",
    "    return pdtrain\n",
    "\n",
    "def remove_last_columns(df):\n",
    "    while not list(df.columns)[-1] in [\"UserId\", \"rating\"]:\n",
    "        df.drop(df.columns[len(df.columns)-1], axis=1, inplace=True)\n",
    "    return df\n",
    "\n",
    "def cast_column_to(df, colname, _type):\n",
    "    \n",
    "    # df.transform(lambda x: x.fillna(x.mean()))\n",
    "\n",
    "    df[colname] = df[colname].astype(_type)\n",
    "    \n",
    "    return df\n",
    "\n",
    "def load_data_to_pd(relative_data_path='./original-beer-recipes/recipeData.csv'):\n",
    "    \n",
    "    target = None  # might there be no target at test time?\n",
    "    \n",
    "    pdtrain = pd.read_csv(relative_data_path, index_col='BeerID', encoding='latin1')\n",
    "    # print(pdtrain.columns)\n",
    "    # Index(['Name', 'URL', 'Style', 'StyleID', 'Size(L)', 'OG', 'FG', 'ABV', 'IBU',\n",
    "    #    'Color', 'BoilSize', 'BoilTime', 'BoilGravity', 'Efficiency',\n",
    "    #    'MashThickness', 'SugarScale', 'BrewMethod', 'PitchRate', 'PrimaryTemp',\n",
    "    #    'PrimingMethod', 'PrimingAmount', 'UserId', 'rating', '\\t'],\n",
    "    #   dtype='object')\n",
    "    pdtrain = remove_last_columns(pdtrain)\n",
    "    \n",
    "    pdtrain = pdtrain.drop(['URL'], axis=1)\n",
    "    pdtrain = pdtrain.drop(['Name'], axis=1)\n",
    "    \n",
    "    with pd.option_context('display.max_rows', None, 'display.max_columns', None):\n",
    "        pass #print(pdtrain['StyleID'])\n",
    "        # pdtrain['StyleID'] = pdtrain['StyleID'].astype(int)\n",
    "    pdtrain = pdtrain.drop(['StyleID'], axis=1)\n",
    "    pdtrain = pdtrain.drop(['Style'], axis=1)\n",
    "    \n",
    "    pdtrain = cast_column_to(pdtrain, 'Size(L)', float)\n",
    "    pdtrain = cast_column_to(pdtrain, 'OG', float)\n",
    "    pdtrain = cast_column_to(pdtrain, 'FG', float)\n",
    "    pdtrain = cast_column_to(pdtrain, 'ABV', float)\n",
    "    pdtrain = cast_column_to(pdtrain, 'IBU', float)\n",
    "    pdtrain = cast_column_to(pdtrain, 'Color', float)\n",
    "    pdtrain = cast_column_to(pdtrain, 'BoilSize', float)\n",
    "    pdtrain = cast_column_to(pdtrain, 'BoilGravity', float)\n",
    "    pdtrain = cast_column_to(pdtrain, 'Efficiency', float)\n",
    "    pdtrain = cast_column_to(pdtrain, 'Color', float)\n",
    "    pdtrain = cast_column_to(pdtrain, 'MashThickness', float)\n",
    "    \n",
    "    # todo: SugarScale, BrewMethod PrimingMethod'PrimingAmount'\n",
    "    pdtrain = pdtrain.drop(['SugarScale'], axis=1)\n",
    "    pdtrain = pdtrain.drop(['BrewMethod'], axis=1)\n",
    "    # pdtrain = cast_column_to(pdtrain, 'PitchRate', float)\n",
    "    pdtrain = pdtrain.drop(['PitchRate'], axis=1)\n",
    "    # pdtrain = cast_column_to(pdtrain, 'PrimaryTemp', float)\n",
    "    pdtrain = pdtrain.drop(['PrimaryTemp'], axis=1)\n",
    "    pdtrain = pdtrain.drop(['PrimingMethod'], axis=1)\n",
    "    pdtrain = pdtrain.drop(['PrimingAmount'], axis=1)\n",
    "    # pdtrain = cast_column_to(pdtrain, 'UserId', int)\n",
    "    pdtrain = pdtrain.drop(['UserId'], axis=1)\n",
    "    pdtrain = cast_column_to(pdtrain, 'rating', float)\n",
    "    \n",
    "    # bad cols: \n",
    "    # StyleID, SugarScale, BrewMethod, PitchRate, PrimingMethod, PrimingAmount, UserId\n",
    "    \n",
    "    # Style    ,StyleID, Size(L),   OG,FG   ,ABV ,IBU  ,Color,BoilSize,BoilTime,BoilGravity,Efficiency,MashThickness,\n",
    "    # Cream Ale,45     , 21.77  ,1.055,1.013,5.48,17.65,4.83 ,28.39   ,75      ,1.038      ,70        ,N/A          ,\n",
    "    \n",
    "    # SugarScale      ,BrewMethod,PitchRate,PrimaryTemp,PrimingMethod,PrimingAmount,UserId,rating,\n",
    "    # Specific Gravity,All Grain ,N/A      ,17.78      ,corn sugar   ,4.5 oz       ,116   ,6,\n",
    "    # Specific Gravity,All Grain, N/A,      N/A        ,corn sugar   ,4.2 oz,        116,0,\n",
    "    # Specific Gravity,BIAB     ,0.35       ,17        ,sucrose        ,140 g,82450,0,\n",
    "\n",
    "\n",
    "    try: \n",
    "        pdtrain = pdtrain[np.isfinite(pdtrain['rating'])]\n",
    "        # pd.DataFrame.dropna(pdtrain).shape\n",
    "        target = pd.DataFrame(pdtrain['rating'])\n",
    "        pdtrain = pdtrain.drop(['rating'], axis=1)\n",
    "    except: \n",
    "        print(\"No test data found in last column named 'rating': can't evaluate!!!\")\n",
    "        pass\n",
    "    \n",
    "    # pdtrain.head()\n",
    "    \n",
    "    # print(pdtrain.columns)\n",
    "    # print(target.columns)\n",
    "    \n",
    "    # print(pdtrain.rating)\n",
    "    # print(pdtrain[\"Unnamed: 24\"])\n",
    "\n",
    "    # print(\"Data set's shape,\")\n",
    "    # print(\"X.shape, integer_y.shape, len(attrs_names), len(output_labels):\")\n",
    "    # print(X.shape, integer_y.shape, len(attrs_names), len(output_labels))\n",
    "    # (1728, 27) (1728,) 27 4\n",
    "\n",
    "    # # Shaping the data into a single pandas dataframe for naming columns:\n",
    "    # pdtrain = pd.DataFrame(X)\n",
    "    # pdtrain.columns = attrs_names\n",
    "    # dtrain = xgb.DMatrix(pdtrain, integer_y)\n",
    "    \n",
    "    # xgtrain = xgb.DMatrix(train.values, target.values)\n",
    "    dtrain = xgb.DMatrix(pdtrain, target)\n",
    "    \n",
    "    target = np.array(target.values).flatten()\n",
    "    return pdtrain, target, dtrain\n",
    "\n",
    "pdtrain, target, dtrain = load_data_to_pd()\n",
    "print(pdtrain.values.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define the features and preprocess the car evaluation data set\n",
    "\n",
    "We'll preprocess the attributes into redundant features, such as using an integer index (linear) to represent a value for an attribute, as well as also using a one-hot encoding for each attribute's possible values as new features. Despite the fact that this is redundant, this will help to make the tree smaller since it has more choice on how to split data on each branch. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train simple decision trees (here using XGBoost) to fit the data set:\n",
    "\n",
    "First, let's define some hyperparameters, such as the depth of the tree."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nxgb1 = xgb.XGBRegressor()\\n\\nparameters = {\\n    \"max_depth\": [1, 3],\\n    \"n_estimators\": [1, 3],\\n    \"learning_rate\": [0.9, 1]\\n    # \"reg_alpha\": [0, 1e-2, 1],\\n    # \"reg_lambda\": [0, 1e-2, 1]\\n}\\n\\nimport sklearn\\nxgb_grid = GridSearchCV(xgb1,\\n                        parameters,\\n                        cv = 2,\\n                        verbose=True, \\n                       scoring=sklearn.metrics.make_scorer(sklearn.metrics.r2_score))\\n\\n# pdtrain = pdtrain.fillna(pdtrain.median()).clip(-1e6,1e6)\\n# pdtrain = pdtrain.reset_index()\\n\\npdtrain = pdtrain.fillna(0)\\nxgb_grid.fit(pdtrain.values, target)\\n'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "xgb1 = xgb.XGBRegressor()\n",
    "\n",
    "parameters = {\n",
    "    \"max_depth\": [1, 3],\n",
    "    \"n_estimators\": [1, 3],\n",
    "    \"learning_rate\": [0.9, 1]\n",
    "    # \"reg_alpha\": [0, 1e-2, 1],\n",
    "    # \"reg_lambda\": [0, 1e-2, 1]\n",
    "}\n",
    "\n",
    "import sklearn\n",
    "xgb_grid = GridSearchCV(xgb1,\n",
    "                        parameters,\n",
    "                        cv = 2,\n",
    "                        verbose=True, \n",
    "                       scoring=sklearn.metrics.make_scorer(sklearn.metrics.r2_score))\n",
    "\n",
    "# pdtrain = pdtrain.fillna(pdtrain.median()).clip(-1e6,1e6)\n",
    "# pdtrain = pdtrain.reset_index()\n",
    "\n",
    "pdtrain = pdtrain.fillna(0)\n",
    "xgb_grid.fit(pdtrain.values, target)\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# best_parameters = xgb_grid.best_estimator_.get_params()\n",
    "# print(best_parameters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dir(xgb.XGBRegressor)\n",
    "\n",
    "trees = xgb.XGBRegressor(max_depth=20, num_boost_round=10, learning_rate=1.43)\n",
    "# trees = xgb.XGBRegressor(**best_parameters)\n",
    "\n",
    "trees.fit(pdtrain, target)\n",
    "\n",
    "got_targets = trees.predict(pdtrain)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mse score (mean squared error score):\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.05049972042299052"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"mse score (mean squared error score):\")\n",
    "np.array((got_targets - target)**2).mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test on other data\n",
    "\n",
    "for now, the other data is the same as train ! but eventually, it'll be your test data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Here are your test data files: ['./new-beer-recipes-here/exampleRecipeData3.csv', './new-beer-recipes-here/exampleRecipeData2.csv', './new-beer-recipes-here/exampleRecipeData1.csv']\n"
     ]
    }
   ],
   "source": [
    "\n",
    "test_datasets_paths = glob.glob(\"./new-beer-recipes-here/*\")\n",
    "\n",
    "print(\"Here are your test data files:\", test_datasets_paths)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TESTING FILE: ./new-beer-recipes-here/exampleRecipeData3.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/IPython/core/interactiveshell.py:2903: DtypeWarning: Columns (4,18,22) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  if self.run_code(code, result):\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mse score (mean squared error score): 0.13548294245926867\n",
      "\n",
      "TESTING FILE: ./new-beer-recipes-here/exampleRecipeData2.csv\n",
      "mse score (mean squared error score): 0.13548294245926867\n",
      "\n",
      "TESTING FILE: ./new-beer-recipes-here/exampleRecipeData1.csv\n",
      "mse score (mean squared error score): 0.13548294245926867\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for file in test_datasets_paths: \n",
    "    \n",
    "    print(\"TESTING FILE:\", file)\n",
    "    \n",
    "    # Note: loading might fail because of errors related to offset columns in CSV: during data loading, I needed to discard columns do to them missing a comma or having too many commas. weird. \n",
    "    pdtest, target_t, dtest = load_data_to_pd(file)\n",
    "    \n",
    "    got_targets = trees.predict(pdtest)\n",
    "    \n",
    "    print(\"mse score (mean squared error score):\", np.array((got_targets - target_t)**2).mean())\n",
    "    \n",
    "    print(\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
